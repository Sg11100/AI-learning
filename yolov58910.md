 我通过阅读[YOLOV10 TO ITS GENESIS:ADECADAL AND COMPREHENSIVE REVIEW OF THE YOU ONLY LOOK ONCE (YOLO) SERIES](https://arxiv.org/abs/2406.19407) 了解了大体的yolo的发展历程和每代的革新之处，并通过进一步搜索对具体实现有了更深刻的了解。





# yolov5

YOLOv5 has significantly contributed to the YOLO series evolution, focusing on **user-friendliness and performance enhancements** . Its introduction by Ultralytics brought a streamlined, accessible framework that lowered the barriers to implementing high-speed object detection across various platforms. 

yolov5是第一个全部用pytorch实现的yolo算法，极大提高了易用性和集成性，简化了模型的复杂程度，便于更好的调整和部署。

YOLOv5’s architecture incorporates a series of optimizations including improved backbone, neck, and head designs which collectively enhance its detection capabilities. The model supports multiple size variants, facilitating a broad range of applications from mobile devices to cloud-based systems. YOLOv5’s adaptability is further evidenced by its **continuous updates and community-driven enhancements**, which ensure it remains at the forefront of object detection technologies.**This version stands out for its balance of speed, accuracy, and utility, making it a preferred choice for developers and researchers looking to deploy state-of-the-art detection systems efficiently**.

 YOLOv5 marks a significant evolution in the YOLO series, focusing on production-ready deployments with streamlined architecture for real-world applications. This version emphasizes reducing the model’s complexity by refining its layers and components, enhancing its inference speed without sacrificing detection accuracy. The backbone and feature extraction layers were optimized to accelerate processing, and the network’s architecture was simplified to facilitate faster data throughput. I

论文并没有提及详细的yolov5的革新之处，于是我又经过自行搜索，得到以下几条特点。

### Mosaic Augmentation 

yolov5在数据增强上采用了mosaic方法，这是前代所没有过的，

mosaic augmentation 将多张照片裁剪后拼接在一起，使模型在更小的范围内识别目标，一般使用四张图片进行拼接。由于该算法由图像拼接而成，所以每张图片会有更大可能性包含**小目标**，这样会使得小目标检测的性能有所提高。也可以增强鲁棒性。

### CIOU损失函数

$$
\mathrm{Loss}_{{\mathrm{CIoU}}}=1-\mathrm{IoU}+\frac{\rho^{2}(\mathrm{b},\mathrm{b}^{{\mathrm{gt}}})}{\mathrm{c}^{2}}+\alpha\mathrm{v}
$$

$$
\begin{aligned}\mathrm{v}&=\frac{4}{\pi^{2}}(\mathrm{arctan}\frac{\mathrm{w}^{\mathrm{g}\mathrm{t}}}{\mathrm{h}^{\mathrm{gt}}}-\mathrm{arctan}\frac{\mathrm{w}}{\mathrm{h}})^{2}\\\\\\\\\alpha&=\frac{\mathrm{v}}{(1-\mathrm{IoU})+\mathrm{v}}\end{aligned}
$$



CIOU在IOU损失函数的基础上增加了两项，第一项为根据**预测框和真实框的中心点的偏差**而增加的惩罚，第二项为根据预测框和真实框长宽比的相似性而添加loss。

# yolov8



YOLOv8 was released in January 2023 by Ultralytics, marking a significant progression in the YOLO series with **an introduction of multiple scaled versions designed to cater to a wide range of applications** . These versions included YOLOv8n (nano), YOLOv8s (small), YOLOv8m (medium), YOLOv8l (large), and YOLOv8x (extra-large), each optimized for specific performance and computational needs. This flexibility made YOLOv8 highly versatile, supporting a multitude of vision tasks such as object detection, segmentation, pose estimation, tracking, and classification, significantly broadening its application scope in real-world scenarios . 

yolov8的多个版本，不同大小和复杂程度让他能够满足不同的需求，完成不同规模的任务。

**The architecture of YOLOv8 underwent substantial refinements to enhance its detection capabilities. It retained a similar backbone to YOLOv5 but introduced modifications in the CSP Layer, now evolved into the C2f module—a cross-stage partial bottleneck with dual convolutions that effectively combine high-level features with contextual information to bolster detection accuracy.** 

c2f（CSP Bottleneck with 2 convolutions）取代c3可有效地将高级特征与上下文信息结合起来，以提高检测精度。

C2f 主要包括：

1. 1 个 1 x 1 的 Conv 层
2. n 个 Bottleneck 层，其中有 shortcut 参数。（？）（而c3一般只有1个）
3. 连接后的 Conv 层

YOLOv8 transitioned to an **anchor-free model** with a.**decoupled head **, allowing independent processing of objectness, classification, and regression tasks which, in turn, improved overall model accuracy . The output layer employed a **sigmoid activation function** for objectness scores and **softmax** for class probabilities, enhancing the precision of bounding box predictions. 

> Anchor-Free 并不是没有使用锚点，而是指**无先验**锚框，直接通过预测具体的点得到锚框。Anchor-Free 不需要手动设计 anchor（长宽比、尺度大小、anchor的数量），从而避免了针对不同数据集进行繁琐的设计。
>
> 尽管 YOLOv8 在检测（推理）阶段被归类为无锚点模型，因为它不依赖于预定义的锚框。
>
> 1. 在训练阶段，它仍然使用了锚点的概念。这些“锚点”作为边界框的尺度和长宽比的**初始估计或参考**。在训练过程中，模型根据训练图像中对象的真实边界框来调整和优化这些估计。
> 2. 每个特征图上的每个点，会定义为 1 个锚框（anchor），模型的输出预测值中包括每个锚框的分类和定位信息。
> 3. 在检测阶段，模型并不严格依赖预定义的锚框来提出候选对象位置，最终的对象检测是直接基于检测到的特征进行的，因此 YOLOv8 被归类为无锚点（anchor-free）模型。
>
> YOLOv8 将输入图像分割成单元格网格，其中每个单元格负责预测位于其中的对象。对于每个单元格，YOLOv8 预测对象得分（objectness scores）、类别概率（class probabilities）和几何偏移量（geometrical offsets），以便估计对象的边界框。
>
> (具体待补充)

**Head**部分使用了解耦头结构(`Decoupled-Head`)，将分类和检测头分离,便于分别独立分类预测，分类使用softman，预测使用sigmoid。

YOLOv8 also integrated advanced loss functions like **CIoU  and Distribution Focal Loss (DFL)** for bounding-box optimization and binary cross-entropy for classification, which proved particularly effective in enhancing detection performance for smaller objects. YOLOv8’s architecture, demonstrated in detailed diagrams, features the modified **CSPDarknet53** backbone with the innovative **C2f** module, augmented **by a spatial pyramid pooling fast (SPPF) layer that accelerates computation by pooling features into a fixed-size map. **

### DFL:

DFL)采用从锚点到边界框四边的相对偏移量作为回归目标，单个偏移量用一般分布（General distribution）表示。

$\mathbf{DFL}(\mathcal{S}_i,\mathcal{S}_{i+1})=-\left((y_{i+1}-y)\log(\mathcal{S}_i)+(y-y_i)\log(\mathcal{S}_{i+1})\right)$

![e7d5f2c62de717daa5c98aa7e91099c0](assets/e7d5f2c62de717daa5c98aa7e91099c0.png)





使用CIoU和DLF来进行边界框回归，使用BCE来进行分类；

同时使用**基于中心点的Anchor-Free**

基于中心点的目标检测方法是对特征图的每个位置预测它是目标中心点的概率, 并且在没有锚框先验的情况下进行边框的预测. 

# yolov9



YOLOv9 marks a significant advancement in real-time object detection by addressing the efficiency and accuracy challenges associated with earlier versions, particularly through the mitigation of information loss in deep neural processing. **It introduces the innovative Programmable Gradient Information (PGI) and the Generalized Efficient Layer Aggregation Network (GELAN) architecture. **These enhancements focus on preserving crucial information across the network, ensuring robust and reliable gradients that prevent data degradation, which is common in deep neural networks .  

PGI和GELAN的使用是yolov9的一大创新之处。

### PGI:

While YOLOv9’s PGI strategically maintains data integrity throughout the processing layers, YOLOv10 builds upon this foundation by completely eliminating the need for NMS and further optimizing model architecture for reduced latency and enhanced computational efficiency. YOLOv10 also introduces dual assignment strategies for NMS-free training, significantly enhancing the system’s response time without compromising accuracy, which reflects a direct evolution from the groundwork laid by YOLOv9’s innovations. 

当数据通过网络的连续层时，信息丢失的可能性会增加。YOLOv9 通过实施可编程梯度信息（PGI）应对了这一挑战，PGI 有助于保留整个网络深度的重要数据，确保更可靠的梯度生成，从而提高模型的收敛性和性能

PGI 是 YOLOv9 为解决信息瓶颈问题而引入的一个新概念，可确保在深层网络中保留重要数据。通过添加了一个辅助可逆分支，该分支用于产生“可信梯度”。如下图所示.这样就能生成可靠的梯度，促进模型的准确更新，提高整体检测性能。

**辅助可逆分支**：一个附加的网络分支，与主干网络并行。可逆分支的目的是在不增加推理成本的情况下，为主干网络提供额外的梯度信息(通过引入多种辅助损失函数提供更多信息）,将浅层特征融合到深层特征中，从而缓解信息瓶颈问题。

前向传播时同主干一起传播，一起计算loss，反向传播时，分别计算各自部分的梯度，再将辅助分支的梯度通过可逆的操作传递到主干上，进行优化。

![image-20240921214543849](assets/image-20240921214543849.png)

### GELAN：

  Furthermore, YOLOv9’s GELAN architecture represents a pivotal improvement in network design, offering a flexible and efficient structure that effectively integrates multi-scale features. While GELAN contributes significantly to YOLOv9’s performance, YOLOv10 extends these architectural improvements to achieve even greater efficiency and adaptability . It reduces computational overhead and increases the model’s applicability to various real-time scenarios, showcasing an advanced level of refinement that leverages and enhances the capabilities introduced by YOLOv9.

GELAN将cspnet和elan融合，得出了全新的架构。

![image-20240917201924050](assets/image-20240917201924050.png)





#### 可以看到文章指出，yolov9有着许多改进，而这些改进有很多都为yolov10奠定了基础。



# yolov10

YOLOv10, developed at Tsinghua University, China, represents a breakthrough in the YOLO series for real-time object detection, achieving unprecedented performance. This version **eliminates the need for non-maximum suppression(NMS) ，a traditional bottleneck in earlier models, thereby drastically reducing latency. **

YOLOv10 **introduces a dual assignment strategy in its training protocol**, which optimizes detection accuracy without sacrificing speed with the help of one-to-many and one-to-one label assignments, ensuring robust detection with lower latency . 

**一种具有双标签分配和一致匹配度量的无nms的yolo训练策略**。

一对多头：在训练过程中为每个对象生成多个预测，以提供丰富的监督信号并提高学习准确性。

一对一头：在推理过程中为每个对象生成一个最佳预测，无需 NMS，从而减少延迟并提高效率。

The architecture of YOLOv10 includes several innovative components that enhance both computational efficiency and detection performance. Among these are lightweight classification heads that reduce computational demands,spatial-channel decoupled downsampling to minimize information loss during feature reduction , and rank guided block design that optimizes parameter use. 

For instance, YOLOv10-S substantially outperforms comparable models (e.g., xxxx) with an improved mAP and lower latency. This version also incorporates holistic efficiency-accuracy driven design, large-kernel convolutions, and partial self-attention modules, which collectively improve the trade-off between computational cost and detection capability. 



![image-20240921212732122](assets/image-20240921212732122.png)

 YOLOv10 的结构建立在以前YOLO 模型的基础上，同时引入了几项关键创新。

主干网：YOLOv10 中的主干网负责特征提取，它使用了增强版的 CSPNet（跨阶段部分网络），以改善梯度流并减少计算冗余。

颈部：颈部设计用于汇聚不同尺度的特征，并将其传递到头部。它包括 PAN（路径聚合网络）层，可实现有效的多尺度特征融合。










