# 深度学习笔记

深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为*深度学习*

# 基础

1. n维数组，也称为*张量*，深度学习框架又比Numpy的`ndarray`多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习。 

   ```python
   x = torch.arange(12)
   ```

   可以通过张量的`shape`属性来访问张量（沿每个轴的长度）的*形状* ，numel访问大小,reshape调整形状

   > 可以使用-1来占据其中一个，由此根据另一个参数调用自动计算出该位置的大小

​	zeros，ones方法创建全0或1的张量，randn创建随机值的张量

2. 运算符可以被应用到元素级
3. cat方法连结多个tensor

```python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```

4. 广播机制

   这种机制的工作方式如下：

   - 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；

   - 对生成的数组执行按元素操作。

     ![image-20240821175545096](../AppData/Roaming/Typora/typora-user-images/image-20240821175545096.png)

5. 原地操作

   ```
   Z[:] = X + Y
   ```

   使用切片法

6. numpy对象和pytorch对象可以相互转换

7. **降维**

​	正常调用sum方法会变为一个标量，但指定轴可以仅仅降一维	

```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```

​	沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。

```python
A.sum(axis=[0, 1])  # 结果和A.sum()相同
```

​	mean 求平均值

​	有时在调用函数来计算总和或均值时保持轴数不变会很有用。

```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```

8. 在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。 当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`时，会执行矩阵-向量积。 注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。

9. 矩阵乘法使用`mm`方法

   

## 微积分

![image-20240821202921044](../AppData/Roaming/Typora/typora-user-images/image-20240821202921044.png)

   ### 自动微分

```python
x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad  # 默认值是None
y = 2 * torch.dot(x, x)
y.backward()
x.grad//y对应的梯度
```

   

```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```

#### 分离计算

有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。 想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数， 并且只考虑到`x`在`y`被计算后发挥的作用。

这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值， 但丢弃计算图中如何计算`y`的任何信息。 换句话说，梯度不会向后流经`u`到`x`。 因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理， 而不是`z=x*x*x`关于`x`的偏导数。

```pyhton
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u
```



 > [!TIP]
 >
 > 在运行反向传播函数之后，立即再次运行它，看看会发生什么。





## 线性回归

*回归*（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。



### 矢量化加速

在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

### 简洁实现

我们可以调用框架中现有的API来读取数据。 我们将`features`和`labels`作为API的参数传递，并通过数据迭代器指定`batch_size`。 此外，布尔值`is_train`表示是否希望数据迭代器对象在每个迭代周期内打乱数据。

```python
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)\
batch_size = 10
data_iter = load_array((features, labels), batch_size)
#data_iter为一个迭代器
```

#### 定义模型

对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量`net`，它是一个`Sequential`类的实例。 `Sequential`类将多个层串联在一起。 当给定输入数据时，`Sequential`实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。 在下面的例子中，我们的模型只包含一个层，因此实际上不需要`Sequential`。 但是由于以后几乎所有的模型都是多层的，在这里使用`Sequential`会让你熟悉“标准的流水线”。

在`PyTorch`中，全连接层在`Linear`类中定义。 值得注意的是，我们将两个参数传递到`nn.Linear`中。 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。

```python
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

# nn是神经网络的缩写
from torch import nn
net = nn.Sequential(nn.Linear(2, 1))
#初始化参数
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
#定义损失函数
loss = nn.MSELoss()
#定义优化算法
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
#训练
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    #跟踪损失变化
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
  
w = net[0].weight.data
print('w的估计误差：', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('b的估计误差：', true_b - b)
```

### softmax回归

*独热编码*是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。

与线性回归一样，`softmax`回归也是一个单层神经网络。 由于计算每个输出o1、o2和o3取决于 所有输入x1、x2、x3和x4， 所以`softmax`回归的输出层也是全连接层。

---

要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。

softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。
$$
\hat{\mathbf{y}}=\mathrm{softmax}(\mathbf{o})\quad\text{其中}\quad\hat{y}_j=\frac{\exp(o_j)}{\sum_k\exp(o_k)}
$$
尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个*线性模型*（linear model）。





#### 实现

Computing the softmax requires three steps: 

1. exponentiation of each term; 
2. a sum over each row to compute the normalization constant for each example; 
3. division of each row by its normalization constant, ensuring that the result sums to 1:

```py
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdims=True)
    return X_exp / partition  # The broadcasting mechanism is applied here
```



```py
@d2l.add_to_class(SoftmaxRegressionScratch)
def forward(self, X):
    X = X.reshape((-1, self.W.shape[0]))  # -1 -> auto 
    return softmax(torch.matmul(X, self.W) + self.b)
```

take care to create real test sets, to consult them as infrequently as possible, to account for multiple hypothesis testing when reporting confidence intervals, and to dial up your vigilance more aggressively when the stakes are high and your dataset size is small. When running a series of benchmark challenges, it is often good practice to maintain several test sets so that after each round, the old test set can be demoted to a validation set.

**test set evaluation is the bedrock of modern machine learning research. it is good practice to curate real test sets (or multiple) and to be as conservative as possible about how often they are used.**



---

**by introducing our model-based decisions to the environment, we might break the model.**





## Environment and Distribution Shift

### Types of Distribution Shift

1. covariate shift     x causes y
2. label shift    y causes x
3. concept shift



# Multilayer Perceptrons

*multilayer perceptrons* consist of multiple layers of neurons each fully connected to those in the layer below (from which they receive input) and those above (which they, in turn, influence). 

 

### 1.limitations of linear

### linearity implies the *weaker* assumption of *monotonicity*



### 2.MLP

We can overcome the limitations of linear models by incorporating one or more hidden layers. The easiest way to do this is to stack many fully connected layers on top of one another. Each layer feeds into the layer above it, until we generate outputs. We can think of the first L−1 layers as our representation and the final layer as our linear predictor. This architecture is commonly called a *multilayer perceptron*, often abbreviated as *MLP* 



In order to realize the potential of multilayer architectures, we need one more key ingredient: a nonlinear **activation function σ** to be applied to each hidden unit following the affine transformation. 





>   [!NOTE] 
>
>  a popular choice is the ReLU (rectified linear unit) activation function ([Nair and Hinton, 2010](https://d2l.ai/chapter_references/zreferences.html#id201)) `σ(x)=max(0,x)` operating on its arguments elementwise. The outputs of activation functions σ(⋅) are called *activations*. In general, with activation functions in place, it is no longer possible to collapse our MLP into a linear model



#### activation functions

1. ReLU Function *****

   $$\mathrm{ReLU}(x)=\max(x,0).$$

    ReLU is significantly more amenable to optimization than the sigmoid or the tanh function. 

2. sigmoid

   $$\mathrm{sigmoid}(x)=\frac1{1+\exp(-x)}.$$

3. tanh

   $$\tanh(x)=\frac{1-\exp(-2x)}{1+\exp(-2x)}.$$



### Implementation

  Typically, we choose the layer widths to be divisible by larger powers of 2.**This is computationally efficient due to the way memory is allocated and addressed in hardware.**



In the code below we use `nn.Parameter` to automatically register a class attribute as a parameter to be tracked by `autograd`

```py
class MLPScratch(d2l.Classifier):
    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)
        self.b1 = nn.Parameter(torch.zeros(num_hiddens))
        self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)
        self.b2 = nn.Parameter(torch.zeros(num_outputs))
```

```python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
```

forward:

```py
@d2l.add_to_class(MLPScratch)
def forward(self, X):
    X = X.reshape((-1, self.num_inputs))
    H = relu(torch.matmul(X, self.W1) + self.b1)
    return torch.matmul(H, self.W2) + self.b2
```

```python
model = MLPScratch(num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1)
data = d2l.FashionMNIST(batch_size=256)
trainer = d2l.Trainer(max_epochs=10)
trainer.fit(model, data)
```



训练模型的基本步骤可以概括为以下几个关键阶段：

1. **定义模型**：

   - **创建模型实例**：定义和初始化一个神经网络模型，包括输入层、隐藏层和输出层的结构和参数。
   - **设置超参数**：例如学习率、正则化项等。

2. **准备数据**：

   - **加载数据集**：选择并加载适合任务的数据集（如 Fashion MNIST），并设置数据的批量大小。
   - **数据预处理**：对数据进行必要的预处理，例如归一化、数据增强等（虽然在此代码段中未显示，但通常是必要的步骤）。

3. **配置训练器**：

   - **创建训练器**：初始化一个训练管理对象，设置训练的相关参数，如最大训练轮次（epochs）、优化器等。

4. **训练模型**：

   - 开始训练：使用训练器的 `fit` 方法，将模型和数据传递给训练器，启动训练过程。训练过程包括：

     - **前向传播**：将输入数据传递通过网络，计算预测结果。
     - **计算损失**：使用损失函数计算预测结果与实际标签之间的误差。
  - **反向传播**：计算梯度并更新模型参数，以减少损失。
     - **迭代优化**：重复前向传播、计算损失和反向传播的过程，直到完成所有的训练轮次。

5. **评估和调优**（虽然在这段代码中未涉及，但通常包括）：

   - **模型评估**：使用验证集或测试集评估训练后的模型性能。
   - **调整超参数**：根据评估结果调整模型参数或训练过程中的超参数，优化模型性能。

### 总结

这些步骤构成了机器学习模型的标准训练流程，从模型的定义、数据准备、训练配置到实际训练过程。



---

#### concise inplementation

```python
class MLP(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_hiddens),
                                 nn.ReLU(), nn.LazyLinear(num_outputs))
```

However, you may have noticed that no `forward` method is defined here. In fact, `MLP` inherits the `forward` method from the `Module` class ([Section 3.2.2](https://d2l.ai/chapter_linear-regression/oo-design.html#subsec-oo-design-models)) to simply invoke `self.net(X)` (`X` is input), which is now defined as a sequence of transformations via the `Sequential` class.

training loop:

```python
model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)
trainer.fit(model, data)
```



---

### 5.3 Backward Propagation

#### forward

![image-20240904112802551](../AppData/Roaming/Typora/typora-user-images/image-20240904112802551.png)

####  Backpropagation

 *Backpropagation* refers to the method of calculating the gradient of neural network parameters. The method traverses the network in reverse order, from the output to the input layer, according to the *chain rule* from calculus.The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters.

The objective of backpropagation is to calculate the gradients$$ ∂J/∂W(1)$$  and$$ ∂J/∂W(2). $$ 




$$
\frac{\partial J}{\partial\mathbf{W}^{(2)}}=\mathrm{prod}\left(\frac{\partial J}{\partial\mathbf{o}},\frac{\partial\mathbf{o}}{\partial\mathbf{W}^{(2)}}\right)+\mathrm{prod}\left(\frac{\partial J}{\partial s},\frac{\partial s}{\partial\mathbf{W}^{(2)}}\right)=\frac{\partial J}{\partial\mathbf{o}}\mathbf{h}^\top+\lambda\mathbf{W}^{(2)}.
$$

$$
\frac{\partial J}{\partial\mathbf{W}^{(1)}}=\mathrm{prod}\left(\frac{\partial J}{\partial\mathbf{z}},\frac{\partial\mathbf{z}}{\partial\mathbf{W}^{(1)}}\right)+\mathrm{prod}\left(\frac{\partial J}{\partial s},\frac{\partial s}{\partial\mathbf{W}^{(1)}}\right)=\frac{\partial J}{\partial\mathbf{z}}\mathbf{x}^\top+\lambda\mathbf{W}^{(1)}.
$$

#### Training Neural Networks

Note that backpropagation reuses the stored intermediate values from forward propagation to avoid duplicate calculations. One of the consequences is that we need to retain the intermediate values until backpropagation is complete. This is also one of the reasons why training requires significantly more memory than plain prediction. 

Besides, the size of such intermediate values is roughly proportional to the number of network layers and the batch size. Thus, training deeper networks using larger batch sizes more easily leads to *out-of-memory* errors.



### 5.4 Numerical Stability and Initialization

#### Vanishing and Exploding Gradients

When dealing with probabilities, a common trick is to switch into log-space, i.e., shifting pressure from the mantissa to the exponent of the numerical representation.



The risks posed by unstable gradients go beyond numerical representation. Gradients of unpredictable magnitude also threaten the stability of our optimization algorithms.

We may be facing parameter updates that are either (i) excessively large, destroying our model (the *exploding gradient* problem); or (ii) excessively small (the *vanishing gradient* problem), rendering learning impossible as parameters hardly move on each update.


#### Vanishing Gradients

One frequent culprit causing the vanishing gradient problem is the choice of the activation function σ that is appended following each layer’s linear operations. 

Let’s take a closer look at the sigmoid to see why it can cause vanishing gradients.

![image-20240904131410632](../AppData/Roaming/Typora/typora-user-images/image-20240904131410632.png)

As you can see, the sigmoid’s gradient vanishes both when its inputs are large and when they are small. Moreover, when backpropagating through many layers, unless we are in the Goldilocks zone, where the inputs to many of the sigmoids are close to zero, the gradients of the overall product may vanish. When our network boasts many layers, unless we are careful, the gradient will likely be cut off at some layer. Indeed, this problem used to plague deep network training. Consequently, ReLUs, which are more stable (but less neurally plausible), have emerged as the default choice for practitioners.



### Builders’ Guide

>In this chapter, we will peel back the curtain, digging deeper into the key components of deep learning computation, namely model construction, parameter access and initialization, designing custom layers and blocks, reading and writing models to disk, and leveraging GPUs to achieve dramatic speedups. These insights will move you from *end user* to *power user*, giving you the tools needed to reap the benefits of a mature deep learning library while retaining the flexibility to implement more complex models, including those you invent yourself! While this chapter does not introduce any new models or datasets, the advanced modeling chapters that follow rely heavily on these techniques.



To implement these complex networks, we introduce the concept of a neural network *module*. A module could describe a single layer, a component consisting of multiple layers, or the entire model itself! One benefit of working with the module abstraction is that they can be combined into larger artifacts, often recursively. This is illustrated in [Fig. 6.1.1](https://d2l.ai/chapter_builders-guide/model-construction.html#fig-blocks). By defining code to generate modules of arbitrary complexity on demand, we can write surprisingly compact code and still implement complex neural networks.



**the basic functionality that each module must provide:**

1. Ingest input data as arguments to its forward propagation method.
2. Generate an output by having the forward propagation method return a value. Note that the output may have a different shape from the input. For example, the first fully connected layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256.
3. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation method. Typically this happens automatically.
4. Store and provide access to those parameters necessary for executing the forward propagation computation.
5. Initialize model parameters as needed.



```python
class MLP(nn.Module):
    def __init__(self):
        # Call the constructor of the parent class nn.Module to perform
        # the necessary initialization
        super().__init__()
        self.hidden = nn.LazyLinear(256)
        self.out = nn.LazyLinear(10)

    # Define the forward propagation of the model, that is, how to return the
    # required model output based on the input X
    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
```

A key virtue of the module abstraction is its versatility. We can subclass a module to create layers (such as the fully connected layer class), entire models (such as the `MLP` class above), or various components of intermediate complexity. We exploit this versatility throughout the coming chapters, such as when addressing convolutional neural networks.



---

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # Random weight parameters that will not compute gradients and
        # therefore keep constant during training
        self.rand_weight = torch.rand((20, 20))
        self.linear = nn.LazyLinear(20)

    def forward(self, X):
        X = self.linear(X)
        X = F.relu(X @ self.rand_weight + 1)
        # Reuse the fully connected layer. This is equivalent to sharing
        # parameters with two fully connected layers
        X = self.linear(X)
        # Control flow
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```

Individual layers can be modules. Many layers can comprise a module. Many modules can comprise a module.

A module can contain code. Modules take care of lots of housekeeping, including parameter initialization and backpropagation. Sequential concatenations of layers and modules are handled by the `Sequential` module.





#### 6.2 parameter

```python
net[2].state_dict()
```

```python
OrderedDict([('weight',
              tensor([[-0.1649,  0.0605,  0.1694, -0.2524,  0.3526, -0.3414, -0.2322,  0.0822]])),
             ('bias', tensor([0.0709]))])
```

initialize the parameter:

```python
def init_normal(module):
    if type(module) == nn.Linear:
        nn.init.normal_(module.weight, mean=0, std=0.01)
        nn.init.zeros_(module.bias)

net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
```

```python
def init_constant(module):
    if type(module) == nn.Linear:
        nn.init.constant_(module.weight, 1)
        nn.init.zeros_(module.bias)

net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
```



#### lazy initialization

The trick here is that the framework *defers initialization*, waiting until the first time we pass data through the model, to infer the sizes of each layer on the fly.

Later on, when working with convolutional neural networks, this technique will become even more convenient since the input dimensionality (e.g., the resolution of an image) will affect the dimensionality of each subsequent layer. Hence the ability to set parameters without the need to know, at the time of writing the code, the value of the dimension can greatly simplify the task of specifying and subsequently modifying our models. Next, we go deeper into the mechanics of initialization.



Note that in this case, only the first layer requires lazy initialization, but the framework initializes sequentially. Once all parameter shapes are known, the framework can finally initialize the parameters.



#### GPUs

```python
@d2l.add_to_class(d2l.Trainer)  #@save
def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):
    self.save_hyperparameters()
    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]

@d2l.add_to_class(d2l.Trainer)  #@save
def prepare_batch(self, batch):
    if self.gpus:
        batch = [a.to(self.gpus[0]) for a in batch]
    return batch

@d2l.add_to_class(d2l.Trainer)  #@save
def prepare_model(self, model):
    model.trainer = self
    model.board.xlim = [0, self.max_epochs]
    if self.gpus:
        model.to(self.gpus[0])
    self.model = model
```



## Convolutional Neural Networks



### Invariance

*what Waldo looks like* does not depend upon *where Waldo is located*. 

CNNs systematize this idea of *spatial invariance*, exploiting it to learn useful representations with fewer parameters.

We can now make these intuitions more concrete by enumerating a few desiderata to guide our design of a neural network architecture suitable for computer vision:

1. In the earliest layers, our network should respond similarly to the same patch, regardless of where it appears in the image. This principle is called *translation invariance* (or *translation equivariance*).
2. The earliest layers of the network should focus on local regions, without regard for the contents of the image in distant regions. This is the *locality* principle. Eventually, these local representations can be aggregated to make predictions at the whole image level.
3. As we proceed, deeper layers should be able to capture longer-range features of the image, in a way similar to higher level vision in nature.

### Translation Invariance

This implies that a shift in the input **X** should simply lead to a shift in the hidden representation **H**.As such, we have **[V]i,j,a,b=[V]a,b** and **U** is a constant, say u. As a result, we can simplify the definition for H:
$$
[\mathbf{H}]_{i,j}=u+\sum_a\sum_b[\mathbf{V}]_{a,b}[\mathbf{X}]_{i+a,j+b}.
$$
Note that$$[\mathrm{V}]_{a,b}$$ needs many fewer coefficients than$$[\mathrm{V}]_{i,j,a,b}$$since it no longer depends on the location within the image.

### Locality

As motivated above, we believe that we should not have to look very far away from location (i,j) in order to glean relevant information to assess what is going on at $$[\mathbf{H}]_{i,j}$$. 

 This means that outside some range $|a|>\Delta$ or $|b|>\Delta$, we should set$[\mathbf{V}]_{a,b}=0.$ Equivalently, we can rewrite $[\mathbf{H}]_{i,j}$ as
$$
[\mathbf{H}]_{i,j}=u+\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^\Delta[\mathbf{V}]_{a,b}[\mathbf{X}]_{i+a,j+b}.
$$


*Convolutional neural networks* (CNNs) are a special family of neural networks that contain convolutional layers. In the deep learning research community, $V$ is referred to as a *convolution kernel*, a *filter*, or simply the layer’s *weights* that are learnable parameters.

The price paid for this drastic reduction in parameters is that our features are now translation invariant and that our layer can only incorporate local information, when determining the value of each hidden activation. All learning depends on imposing inductive bias. When that bias agrees with reality, we get sample-efficient models that generalize well to unseen data. But of course, if those biases do not agree with reality, e.g., if images turned out not to be translation invariant, our models might struggle even to fit our training data.

This dramatic reduction in parameters brings us to our last desideratum, namely that deeper layers should represent larger and more complex aspects of an image. This can be achieved by interleaving nonlinearities and convolutional layers repeatedly.

### Channels

We could think of the hidden representations as comprising a number of two-dimensional grids stacked on top of each other. As in the inputs, these are sometimes called *channels*. They are also sometimes called *feature maps*, as each provides a spatialized set of learned features for the subsequent layer.

To support multiple channels in both inputs (X) and hidden representations (H), we can add a fourth coordinate to V: [V]a,b,c,d. Putting everything together we have

$$[\mathsf{H}]_{i,j,d}=\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^\Delta\sum_c[\mathsf{V}]_{a,b,c,d}[\mathsf{X}]_{i+a,j+b,c},$$

Adding channels allowed us to bring back some of the complexity that was lost due to the restrictions imposed on the convolutional kernel by locality and translation invariance. Note that it is quite natural to add channels other than just red, green, and blue.Many satellite images, in particular for agriculture and meteorology, have tens to hundreds of channels, generating hyperspectral images instead. They report data on many different wavelengths. 



## Convolutions for Images

Note that along each axis, the output size is slightly smaller than the input size. 

Because the kernel has width and height greater than 1, we can only properly compute the cross-correlation for locations where the kernel fits wholly within the image,



```python
def corr2d(X, K):  #@save
    """Compute 2D cross-correlation."""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
```

### Convolutional Layers

A convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an output. The two parameters of a convolutional layer are the kernel and the scalar bias. When training models based on convolutional layers, we typically initialize the kernels randomly, just as we would with a fully connected layer.

```python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

### Learning a Kernel

```python
In order to obtain the output of the strict convolution operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the cross-correlation operation with the input tensor.# Construct a two-dimensional convolutional layer with 1 output channel and a
# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here
conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)

# The two-dimensional convolutional layer uses four-dimensional input and
# output in the format of (example, channel, height, width), where the batch
# size (number of examples in the batch) and the number of channels are both 1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2  # Learning rate

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # Update the kernel
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i + 1}, loss {l.sum():.3f}')
        
conv2d.weight.data.reshape((1, 2))
```

In order to obtain the output of the strict *convolution* operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the *cross-correlation* operation with the input tensor.

It is noteworthy that since kernels are learned from data in deep learning, the outputs of convolutional layers remain unaffected no matter such layers perform either the strict convolution operations or the cross-correlation operations.

In keeping with standard terminology in deep learning literature, we will continue to refer to the cross-correlation operation as a convolution even though, strictly-speaking, it is slightly different. Furthermore, we use the term *element* to refer to an entry (or component) of any tensor representing a layer representation or a convolution kernel.

### Feature Map and Receptive Field

In CNNs, for any element x of some layer, its *receptive field* refers to all the elements (from all the previous layers) that may affect the calculation of x during the forward propagation. Note that the receptive field may be larger than the actual size of the input.

Thus, when any element in a feature map needs a larger receptive field to detect input features over a broader area, we can build a deeper network.

> [!IMPORTANT] 
>
> In terms of convolutions themselves, they can be used for many purposes, for example detecting edges and lines, blurring images, or sharpening them. Most importantly, it is not necessary that the statistician (or engineer) invents suitable filters. Instead, we can simply *learn* them from data. This replaces feature engineering heuristics by evidence-based statistics.

## padding and stride

### padding

Since we typically use small kernels, for any given convolution we might only lose a few pixels but this can add up as we apply many successive convolutional layers. One straightforward solution to this problem is to add extra pixels of filler around the boundary of our input image, thus increasing the effective size of the image. 

In many cases, we will want to set $p_\mathrm{h}=k_\mathrm{h}-1$ and $p_\mathrm{w}=k_\mathrm{w}-1$ to give the input and output 

**the same height and width** . This will make it easier to predict the output shape of each layer when constructing the network. Assuming that $k_\mathrm{h}$ is odd here, we will pad $p_\mathrm{h}/2$ rows on both sides of the height.If $k_\mathrm{h}$ is even, one possibility is to pad $\lceil p_\mathrm{h}/2\rceil$ rows on the top of the input and $\lfloor p_\mathrm{h}/2\rfloor$ rows on the bottom. We will pad both sides of the width in the same way.



CNNs commonly use convolution kernels with odd height and width values, such as 1, 3, 5, or 7. Choosing odd kernel sizes has the benefit that we can preserve the dimensionality while padding with the same number of rows on top and bottom, and the same number of columns on left and right.

In the following example, we create a two-dimensional convolutional layer with a height and width of 3 and apply 1 pixel of padding on all sides. Given an input with a height and width of 8, we find that the height and width of the output is also 8.

```python
# We define a helper function to calculate convolutions. It initializes the
# convolutional layer weights and performs corresponding dimensionality
# elevations and reductions on the input and output
def comp_conv2d(conv2d, X):
    # (1, 1) indicates that batch size and the number of channels are both 1
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    # Strip the first two dimensions: examples and channels
    return Y.reshape(Y.shape[2:])

# 1 row and column is padded on either side, so a total of 2 rows or columns
# are added
conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape
```

When the height and width of the convolution kernel are different, we can make the output and input have the same height and width by setting different padding numbers for height and width.

```python
# We use a convolution kernel with height 5 and width 3. The padding on either
# side of the height and width are 2 and 1, respectively
conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape
```

### stride

Sometimes, either for computational efficiency or because we wish to downsample, we move our window more than one element at a time, skipping the intermediate locations. This is particularly useful if the convolution kernel is large since it captures a large area of the underlying image.



In general, when the stride for the height is $s_\mathrm{h}$ and the stride for the width is $s_\mathrm{w}$, the output shape is
$$
\lfloor(n_\mathrm{h}-k_\mathrm{h}+p_\mathrm{h}+s_\mathrm{h})/s_\mathrm{h}\rfloor\times\lfloor(n_\mathrm{w}-k_\mathrm{w}+p_\mathrm{w}+s_\mathrm{w})/s_\mathrm{w}\rfloor.
$$
If we set $p_\mathrm{h}=k_\mathrm{h}-1$ and $p_\mathrm{w}=k_\mathrm{w}-1$,then the output shape can be simplified to
$\lfloor(n_\mathrm{h}+s_\mathrm{h}-1)/s_\mathrm{h}\rfloor\times\lfloor(n_\mathrm{w}+s_\mathrm{w}-1)/s_\mathrm{w}\rfloor$. Going a step further, if the input height and width
are divisible by the strides on the height and width, then the output shape will be
$(n_\mathrm{h}/s_\mathrm{h})\times(n_\mathrm{w}/s_\mathrm{w}).$



## Multiple Input and Multiple Output Channels

However, when ci>1, we need a kernel that contains a tensor of shape kh×kw for *every* input channel. Concatenating these ci tensors together yields a convolution kernel of shape ci×kh×kw. Since the input and convolution kernel each have ci channels, we can perform a cross-correlation operation on the two-dimensional tensor of the input and the two-dimensional tensor of the convolution kernel for each channel, adding the ci results together (summing over the channels) to yield a two-dimensional tensor. This is the result of a two-dimensional cross-correlation between a multi-channel input and a multi-input-channel convolution kernel.

```python
def corr2d_multi_in(X, K):
    # Iterate through the 0th dimension (channel) of K first, then add them up
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
```

---

In the most popular neural network architectures, we actually increase the channel dimension as we go deeper in the neural network, typically downsampling to trade off spatial resolution for greater *channel depth*.Intuitively, you could think of each channel as responding to a different set of features. 

Denote by $c_\mathrm{i}$ and $c_\mathrm{o}$ the number of input and output channels, respectively, and by $k_\mathrm{h}$ and $k_\mathrm{w}$ the height and width of the kernel. To get an output with multiple channels, we can create a kernel tensor of shape $c_\mathrm{i}\times k_\mathrm{h}\times k_\mathrm{w}$ for every output channel. We concatenate them on the output
channel dimension, so that the shape of the convolution kernel is $c_\mathrm{o}\times c_\mathrm{i}\times k_\mathrm{h}\times k_\mathrm{w}$. In cross-
correlation operations, the result on each output channel is calculated from the convolution kernel corresponding to that output channel and takes input from all channels in the input tensor.

> [!IMPORTANT]
>
> ````python
>     def corr2d_multi_in_out(X, K):
>     # Iterate through the 0th dimension of K, and each time, perform
>     # cross-correlation operations with input X. All of the results are
>     # stacked together
>     return torch.stack([corr2d_multi_in(X, k) for k in K], 0)
> ````
Channels allow us to combine the best of both worlds: MLPs that allow for significant nonlinearities and convolutions that allow for *localized* analysis of features. In particular, channels allow the CNN to reason with multiple features, such as edge and shape detectors at the same time. They also offer a practical trade-off between the drastic parameter reduction arising from translation invariance and locality, and the need for expressive and diverse models in computer vision.



### Pooling

This section introduces *pooling layers*, which serve the dual purposes of mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations.

#### Maximum Pooling and Average Pooling

Like convolutional layers, *pooling* operators consist of a fixed-shape window that is slid over all regions in the input according to its stride, computing a single output for each location traversed by the fixed-shape window (sometimes known as the *pooling window*). 

However, unlike the cross-correlation computation of the inputs and kernels in the convolutional layer, **the pooling layer contains no parameters (there is no *kernel*). Instead, pooling operators are deterministic, typically calculating either the maximum or the average value of the elements in the pooling window. **These operations are called *maximum pooling* (*max-pooling* for short) and *average pooling*, respectively.

In almost all cases, max-pooling, as it is also referred to, is preferable to average pooling.

In the code below, we implement the forward propagation of the pooling layer in the `pool2d` function.  However, no kernel is needed, computing the output as either the maximum or the average of each region in the input.

```python
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
```

#### Padding and Stride

As with convolutional layers, pooling layers change the output shape. And as before, we can adjust the operation to achieve a desired output shape by padding the input and adjusting the stride.

Since pooling aggregates information from an area, deep learning frameworks default to matching pooling window sizes and stride. For instance, if we use a pooling window of shape `(3, 3)` we get a stride shape of `(3, 3)` by default.

```python
pool2d = nn.MaxPool2d(3)
# Pooling has no model parameters, hence it needs no initialization
pool2d(X)
```
Needless to say, the stride and padding can be manually specified to override framework defaults if required.

```python
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
```

#### Multiple Channels

When processing multi-channel input data, the pooling layer pools each input channel separately, rather than summing the inputs up over channels as in a convolutional layer. This means that the number of output channels for the pooling layer is the same as the number of input channels.

##  Convolutional Neural Networks (LeNet)

At a high level, LeNet (LeNet-5) consists of two parts: (i) a convolutional encoder consisting of two convolutional layers; and (ii) a dense block consisting of three fully connected layers. 



The basic units in each convolutional block are a convolutional layer, a sigmoid activation function, and a subsequent average pooling operation. Note that while ReLUs and max-pooling work better, they had not yet been discovered. Each convolutional layer uses a 5×5 kernel and a sigmoid activation function. These layers map spatially arranged inputs to a number of two-dimensional feature maps, typically increasing the number of channels. The first convolutional layer has 6 output channels, while the second has 16. Each 2×2 pooling operation (stride 2) reduces dimensionality by a factor of 4 via spatial downsampling. The convolutional block emits an output with shape given by (batch size, number of channel, height, width).



In order to pass output from the convolutional block to the dense block, we must flatten each example in the minibatch. In other words, we take this four-dimensional input and transform it into the two-dimensional input expected by fully connected layers.as a reminder, the two-dimensional representation that we desire uses the first dimension to index examples in the minibatch and the second to give the flat vector representation of each example. LeNet’s dense block has three fully connected layers, with 120, 84, and 10 outputs, respectively. Because we are still performing classification, the 10-dimensional output layer corresponds to the number of possible output classes.

![../_images/lenet.svg](https://d2l.ai/_images/lenet.svg)

#### Implement:

```python
def init_cnn(module):  #@save
    """Initialize weights for CNNs."""
    if type(module) == nn.Linear or type(module) == nn.Conv2d:
        nn.init.xavier_uniform_(module.weight)

class LeNet(d2l.Classifier):  #@save
    """The LeNet-5 model."""
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),
            nn.AvgPool2d(kernel_size=2, stride=2),
            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),
            nn.AvgPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.LazyLinear(120), nn.Sigmoid(),
            nn.LazyLinear(84), nn.Sigmoid(),
            nn.LazyLinear(num_classes))
```



![../_images/lenet-vert.svg](https://d2l.ai/_images/lenet-vert.svg)

```python
@d2l.add_to_class(d2l.Classifier)  #@save
def layer_summary(self, X_shape):
    X = torch.randn(*X_shape)
    for layer in self.net:
        X = layer(X)
        print(layer.__class__.__name__, 'output shape:\t', X.shape)

model = LeNet()
model.layer_summary((1, 1, 28, 28))
```

```python
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128)
model = LeNet(lr=0.1)
model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)
trainer.fit(model, data)
```



# Modern Convolutional Neural Networks

While the idea of *deep* neural networks is quite simple (stack together a bunch of layers), performance can vary wildly across architectures and hyperparameter choices. 

 Rather than *learning* the features, the features were *crafted*.  Most of the progress came from having more clever ideas for feature extraction on the one hand and deep insight into geometry.The learning algorithm was often considered an afterthought



### 1.AlexNet

Thus, rather than training *end-to-end* (pixel to classification) systems, classical pipelines looked more like this:

1. Obtain an interesting dataset. In the early days, these datasets required expensive sensors. For instance, the [Apple QuickTake 100](https://en.wikipedia.org/wiki/Apple_QuickTake) of 1994 sported a whopping 0.3 megapixel (VGA) resolution, capable of storing up to 8 images, all for the price of $1000.
2. Preprocess the dataset with hand-crafted features based on some knowledge of optics, geometry, other analytic tools, and occasionally on the serendipitous discoveries by lucky graduate students.
3. Feed the data through a standard set of feature extractors such as the SIFT (scale-invariant feature transform) ([Lowe, 2004](https://d2l.ai/chapter_references/zreferences.html#id180)), the SURF (speeded up robust features) ([Bay *et al.*, 2006](https://d2l.ai/chapter_references/zreferences.html#id12)), or any number of other hand-tuned pipelines. OpenCV still provides SIFT extractors to this day!
4. Dump the resulting representations into your favorite classifier, likely a linear model or kernel method, to train a classifier.



> [!IMPORTANT]
>
> Computer vision researchers justifiably believed that a slightly bigger or cleaner dataset or a slightly improved feature-extraction pipeline mattered far more to the final accuracy than any learning algorithm.

### Representation Learning

The first modern CNN ([Krizhevsky *et al.*, 2012](https://d2l.ai/chapter_references/zreferences.html#id155)), named *AlexNet* after one of its inventors, Alex Krizhevsky, is largely an evolutionary improvement over LeNet. 

Interestingly, in the lowest layers of the network, the model learned feature extractors that resembled some traditional filters. 

### AlexNet

![../_images/alexnet.svg](https://d2l.ai/_images/alexnet.svg)

<center>
  From LeNet (left) to AlexNet (right).  
</center>
There are also significant differences between AlexNet and LeNet. First, AlexNet is much deeper than the comparatively small LeNet-5. AlexNet consists of eight layers: five convolutional layers, two fully connected hidden layers, and one fully connected output layer. Second, AlexNet used the ReLU instead of the sigmoid as its activation function. Let’s delve into the details below.

AlexNet controls the model complexity of the fully connected layer by dropout ([Section 5.6](https://d2l.ai/chapter_multilayer-perceptrons/dropout.html#sec-dropout)), while LeNet only uses weight decay. To augment the data even further, the training loop of AlexNet added a great deal of image augmentation, such as flipping, clipping, and color changes. This makes the model more robust and the larger sample size effectively reduces overfitting. See Buslaev *et al.* ([2020](https://d2l.ai/chapter_references/zreferences.html#id31)) for an in-depth review of such preprocessing steps.

```python
class AlexNet(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),
            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),
            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),
            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),
            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),
            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),
            nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),
            nn.LazyLinear(num_classes))
        self.net.apply(d2l.init_cnn)
```



```python
model = AlexNet(lr=0.01)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
trainer.fit(model, data)
```

Now, we can start training AlexNet. Compared to LeNet in [Section 7.6](https://d2l.ai/chapter_convolutional-neural-networks/lenet.html#sec-lenet), the main change here is the use of a smaller learning rate and much slower training due to the deeper and wider network, the higher image resolution, and the more costly convolutions.

Reviewing the architecture, we see that AlexNet has an Achilles heel when it comes to efficiency: the last two hidden layers require matrices of size 6400×4096 and 4096×4096, respectively. This corresponds to 164 MB of memory and 81 MFLOPs of computation, both of which are a nontrivial outlay, especially on smaller devices, such as mobile phones. This is one of the reasons why AlexNet has been surpassed by much more effective architectures that we will cover in the following sections. Nonetheless, it is a key step from shallow to deep networks that are used nowadays. Note that even though the number of parameters exceeds by far the amount of training data in our experiments (the last two layers have more than 40 million parameters, trained on a datasets of 60 thousand images), there is hardly any overfitting: training and validation loss are virtually identical throughout training. This is due to the improved regularization, such as dropout, inherent in modern deep network designs.

### NIN



![../_images/nin.svg](https://d2l.ai/_images/nin.svg)

<center>
    Comparing the architectures of VGG and NiN, and of their blocks.
</center>



### GoogLeNet

![../_images/inception.svg](https://d2l.ai/_images/inception.svg)

<center>
    Structure of the Inception block.
</center>

![../_images/inception-full-90.svg](https://d2l.ai/_images/inception-full-90.svg)

<center>
    The GoogLeNet architecture.
</center>





## Batch Normalization

As it turns out, quite serendipitously, batch normalization conveys all three benefits: **preprocessing, numerical stability, and regularization.**



Batch normalization is applied to individual layers, or optionally, to all of them: In each training iteration, we first normalize the inputs (of batch normalization) by subtracting their mean and dividing by their standard deviation, where both are estimated based on the statistics of the current minibatch. Next, we apply a scale coefficient and an offset to recover the lost degrees of freedom. It is precisely due to this *normalization* based on *batch* statistics that *batch normalization* derives its name.

One takeaway here is that when applying batch normalization, the choice of batch size is even more significant than without batch normalization, or at least, suitable calibration is needed as we might adjust batch size.



Denote by $\mathcal{B}$ a minibatch and let $\mathbf{x}\in\mathcal{B}$ be an input to batch normalization (BN). In this case the
batch normalization is defined as follows:
$$
\mathrm{BN}(\mathbf{x})=\gamma\odot\frac{\mathbf{x}-\hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}}+\beta.
$$
$\hat{\boldsymbol{\mu}}_B$ is the sample mean and $\hat{\boldsymbol{\sigma}}_B$ is the sample standard deviation of the minibatch $\mathcal{B}.$
After applying standardization, the resulting minibatch has zero mean and unit variance. The choice of unit variance (rather than some other magic number) is arbitrary. We recover this degree of freedom by including an elementwise scale parameter $\gamma$ and shift parameter $\beta$ that have the same shape as $\mathbf{x}.$ Both are parameters that need to be learned as part of model training



The variable magnitudes for intermediate layers cannot diverge during training since batch normalization actively centers and rescales them back to a given mean and size. Practical experience confirms that, as alluded to when discussing feature rescaling, batch normalization seems to allow for more aggressive learning rates. 
$$
\hat{\boldsymbol{\mu}}_\mathcal{B}=\frac1{|\mathcal{B}|}\sum_{\mathbf{x}\in\mathcal{B}}\mathbf{x}\mathrm{~and~}\hat{\boldsymbol{\sigma}}_\mathcal{B}^2=\frac1{|\mathcal{B}|}\sum_{\mathbf{x}\in\mathcal{B}}(\mathbf{x}-\hat{\boldsymbol{\mu}}_\mathcal{B})^2+\epsilon.
$$
Note that we add a small constant $ϵ>0 $to the variance estimate to ensure that we never attempt division by zero,

> [!IMPORTANT]
>
>  For reasons that are not yet well-characterized theoretically, various sources of noise in optimization often lead to faster training and less overfitting: this variation appears to act as a form of regularization. Teye *et al.* ([2018](https://d2l.ai/chapter_references/zreferences.html#id281)) and Luo *et al.* ([2018](https://d2l.ai/chapter_references/zreferences.html#id181)) related the properties of batch normalization to Bayesian priors and penalties, respectively. In particular, this sheds some light on the puzzle of why batch normalization works best for moderate minibatch sizes in the 50–100 range. This particular size of minibatch seems to inject just the “right amount” of noise per layer, both in terms of scale via σ^, and in terms of offset via μ^: a larger minibatch regularizes less due to the more stable estimates, whereas tiny minibatches destroy useful signal due to high variance. Exploring this direction further, considering alternative types of preprocessing and filtering may yet lead to other effective types of regularization.
>
> Fixing a trained model, you might think that we would prefer using the entire dataset to estimate the mean and variance. Once training is complete, why would we want the same image to be classified differently, depending on the batch in which it happens to reside? During training, such exact calculation is infeasible because the intermediate variables for all data examples change every time we update our model. However, once the model is trained, we can calculate the means and variances of each layer’s variables based on the entire dataset. Indeed this is standard practice for models employing batch normalization; thus batch normalization layers function differently in *training mode* (normalizing by minibatch statistics) than in *prediction mode* (normalizing by dataset statistics). In this form they closely resemble the behavior of dropout regularization of [Section 5.6](https://d2l.ai/chapter_multilayer-perceptrons/dropout.html#sec-dropout), where noise is only injected during training.

Batch normalization implementations for fully connected layers and convolutional layers are slightly different. One key difference between batch normalization and other layers is that because the former operates on a full minibatch at a time, we cannot just ignore the batch dimension as we did before when introducing other layers.



### Fully Connected Layers

$$
\mathbf{h}=\phi(\mathrm{BN}(\mathbf{Wx}+\mathbf{b})).
$$

### Convolutional Layers

Similarly, with convolutional layers, we can apply batch normalization after the convolution but before the nonlinear activation function. The key difference from batch normalization in fully connected layers is that we apply the operation on a per-channel basis *across all locations*. This is compatible with our assumption of translation invariance that led to convolutions: we assumed that the specific location of a pattern within an image was not critical for the purpose of understanding.

Assume that our minibatches contain m examples and that for each channel, the output of the convolution has height p and width q. For convolutional layers, we carry out each batch normalization over the m⋅p⋅q elements per output channel simultaneously. Thus, we collect the values over all spatial locations when computing the mean and variance and consequently apply the same mean and variance within a given channel to normalize the value at each spatial location. Each channel has its own scale and shift parameters, both of which are scalars.

### Batch Normalization During Prediction

As we mentioned earlier, batch normalization typically behaves differently in training mode than in prediction mode. First, the noise in the sample mean and the sample variance arising from estimating each on minibatches is no longer desirable once we have trained the model. Second, we might not have the luxury of computing per-batch normalization statistics. For example, we might need to apply our model to make one prediction at a time.

Typically, after training, we use the entire dataset to compute stable estimates of the variable statistics and then fix them at prediction time. Hence, batch normalization behaves differently during training than at test time. Recall that dropout also exhibits this characteristic.

### Implementation from Scratch

```python
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # Use is_grad_enabled to determine whether we are in training mode
    if not torch.is_grad_enabled():
        # In prediction mode, use mean and variance obtained by moving average
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # When using a fully connected layer, calculate the mean and
            # variance on the feature dimension
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # When using a two-dimensional convolutional layer, calculate the
            # mean and variance on the channel dimension (axis=1). Here we
            # need to maintain the shape of X, so that the broadcasting
            # operation can be carried out later
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # In training mode, the current mean and variance are used
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # Update the mean and variance using moving average
        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean
        moving_var = (1.0 - momentum) * moving_var + momentum * var
    Y = gamma * X_hat + beta  # Scale and shift
    return Y, moving_mean.data, moving_var.data
```

We can now create a proper `BatchNorm` layer. Our layer will maintain proper parameters for scale `gamma` and shift `beta`, both of which will be updated in the course of training. Additionally, our layer will maintain moving averages of the means and variances for subsequent use during model prediction.



> [!NOTE]
>
> Putting aside the algorithmic details, note the design pattern underlying our implementation of the layer. Typically, we define the mathematics in a separate function, say `batch_norm`. We then integrate this functionality into a custom layer, whose code mostly addresses bookkeeping matters, such as moving data to the right device context, allocating and initializing any required variables, keeping track of moving averages (here for mean and variance), and so on. This pattern enables a clean separation of mathematics from boilerplate code.



```python
class BatchNorm(nn.Module):
    # num_features: the number of outputs for a fully connected layer or the
    # number of output channels for a convolutional layer. num_dims: 2 for a
    # fully connected layer and 4 for a convolutional layer
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # The scale parameter and the shift parameter (model parameters) are
        # initialized to 1 and 0, respectively
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # The variables that are not model parameters are initialized to 0 and
        # 1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # If X is not on the main memory, copy moving_mean and moving_var to
        # the device where X is located
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # Save the updated moving_mean and moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.1)
        return Y
```

### Concise Implementation

```python
class BNLeNet(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),
            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),
            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),
            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),
            nn.Sigmoid(), nn.LazyLinear(num_classes))
```

```python
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128)
model = BNLeNet(lr=0.1)
model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)
```



The original paper proposing batch normalization ([Ioffe and Szegedy, 2015](https://d2l.ai/chapter_references/zreferences.html#id133)), in addition to introducing a powerful and useful tool, offered an explanation for why it works: by reducing *internal covariate shift*. Presumably by *internal covariate shift* they meant something like the intuition expressed above—the notion that the distribution of variable values changes over the course of training.

We note that the *internal covariate shift* is no more worthy of criticism than any of thousands of similarly vague claims made every year in the technical machine learning literature. Likely, its resonance as a focal point of these debates owes to its broad recognizability for the target audience. Batch normalization has proven an indispensable method, applied in nearly all deployed image classifiers, earning the paper that introduced the technique tens of thousands of citations. We conjecture, though, that the guiding principles of regularization through noise injection, acceleration through rescaling and lastly preprocessing may well lead to further inventions of layers and techniques in the future.



### Residual Networks (ResNet) and ResNeXt

![../_images/resnet-block.svg](https://d2l.ai/_images/resnet-block.svg)

 <center>
     Fig. 8.6.3 ResNet block with and without 
 convolution, which transforms the input into the desired shape for the addition operation.
 </center>

```python
class ResNet(d2l.Classifier):
    def b1(self):
        return nn.Sequential(
            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),
            nn.LazyBatchNorm2d(), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

```python
@d2l.add_to_class(ResNet)
def block(self, num_residuals, num_channels, first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels))
    return nn.Sequential(*blk)
```

```python
@d2l.add_to_class(ResNet)
def __init__(self, arch, lr=0.1, num_classes=10):
    super(ResNet, self).__init__()
    self.save_hyperparameters()
    self.net = nn.Sequential(self.b1())
    for i, b in enumerate(arch):
        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))
    self.net.add_module('last', nn.Sequential(
        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
        nn.LazyLinear(num_classes)))
    self.net.apply(d2l.init_cnn)
```

There are four convolutional layers in each module (excluding the 1×1 convolutional layer). Together with the first 7×7 convolutional layer and the final fully connected layer, there are 18 layers in total. Therefore, this model is commonly known as ResNet-18. By configuring different numbers of channels and residual blocks in the module, we can create different ResNet models, such as the deeper 152-layer ResNet-152. Although the main architecture of ResNet is similar to that of GoogLeNet, ResNet’s structure is simpler and easier to modify. All these factors have resulted in the rapid and widespread use of ResNet. [Fig. 8.6.4](https://d2l.ai/chapter_convolutional-modern/resnet.html#fig-resnet18) depicts the full ResNet-18.

![../_images/resnet18-90.svg](https://d2l.ai/_images/resnet18-90.svg)

We train ResNet on the Fashion-MNIST dataset, just like before. ResNet is quite a powerful and flexible architecture. The plot capturing training and validation loss illustrates a significant gap between both graphs, with the training loss being considerably lower. For a network of this flexibility, more training data would offer distinct benefit in closing the gap and improving accuracy.

```python
model = ResNet18(lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)
```



### ResNeXt

 Applying the idea of multiple independent groups to the ResNet block of [Fig. 8.6.3](https://d2l.ai/chapter_convolutional-modern/resnet.html#fig-resnet-block) led to the design of ResNeXt ([Xie *et al.*, 2017](https://d2l.ai/chapter_references/zreferences.html#id321)). Different from the smorgasbord of transformations in Inception, ResNeXt adopts the *same* transformation in all branches, thus minimizing the need for manual tuning of each branch.

![../_images/resnext-block.svg](https://d2l.ai/_images/resnext-block.svg)

<center>
   Fig. 8.6.5 The ResNeXt block. The use of grouped convolution with 
 groups is 
 times faster than a dense convolution. It is a bottleneck residual block when the number of intermediate channels 
 is less than 
. 
</center>

Breaking up a convolution from ci to co channels into one of g groups of size ci/g generating g outputs of size co/g is called, quite fittingly, a *grouped convolution*. The computational cost (proportionally) is reduced from O(ci⋅co) to O(g⋅(ci/g)⋅(co/g))=O(ci⋅co/g), i.e., it is g times faster. Even better, the number of parameters needed to generate the output is also reduced from a ci×co matrix to g smaller matrices of size (ci/g)×(co/g), again a g times reduction. In what follows we assume that both ci and co are divisible by g.

The only challenge in this design is that no information is exchanged between the g groups. The ResNeXt block of [Fig. 8.6.5](https://d2l.ai/chapter_convolutional-modern/resnet.html#fig-resnext-block) amends this in two ways: the grouped convolution with a 3×3 kernel is sandwiched in between two 1×1 convolutions. The second one serves double duty in changing the number of channels back. The benefit is that we only pay the O(c⋅b) cost for 1×1 kernels and can make do with an O(b2/g) cost for 3×3 kernels. Similar to the residual block implementation in [Section 8.6.2](https://d2l.ai/chapter_convolutional-modern/resnet.html#subsec-residual-blks), the residual connection is replaced (thus generalized) by a 1×1 convolution.





The following implementation of the `ResNeXtBlock` class takes as argument `groups` (g), with `bot_channels` (b) intermediate (bottleneck) channels. Lastly, when we need to reduce the height and width of the representation, we add a stride of 2 by setting `use_1x1conv=True, strides=2`.

```python
class ResNeXtBlock(nn.Module):  #@save
    """The ResNeXt block."""
    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,
                 strides=1):
        super().__init__()
        bot_channels = int(round(num_channels * bot_mul))
        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)
        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,
                                   stride=strides, padding=1,
                                   groups=bot_channels//groups)
        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)
        self.bn1 = nn.LazyBatchNorm2d()
        self.bn2 = nn.LazyBatchNorm2d()
        self.bn3 = nn.LazyBatchNorm2d()
        if use_1x1conv:
            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,
                                       stride=strides)
            self.bn4 = nn.LazyBatchNorm2d()
        else:
            self.conv4 = None

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = F.relu(self.bn2(self.conv2(Y)))
        Y = self.bn3(self.conv3(Y))
        if self.conv4:
            X = self.bn4(self.conv4(X))
        return F.relu(Y + X)
```

Nested function classes are desirable since they allow us to obtain strictly *more powerful* rather than also subtly *different* function classes when adding capacity. One way of accomplishing this is by letting additional layers to simply pass through the input to the output. Residual connections allow for this. As a consequence, this changes the inductive bias from simple functions being of the form f(x)=0 to simple functions looking like f(x)=x.

Another benefit of residual networks is that it allows us to add layers, initialized as the identity function, *during* the training process. After all, the default behavior of a layer is to let the data pass through unchanged. This can accelerate the training of very large networks in some cases.

>  [!TIP]
>
> ResNeXt is an example for how the design of convolutional neural networks has evolved over time: by being more frugal with computation and trading it off against the size of the activations (number of channels), it allows for faster and more accurate networks at lower cost. 

### Densely Connected Networks (DenseNet)

ResNet significantly changed the view of how to parametrize the functions in deep networks. *DenseNet* (dense convolutional network) is to some extent the logical extension of this ([Huang *et al.*, 2017](https://d2l.ai/chapter_references/zreferences.html#id127)). DenseNet is characterized by both the connectivity pattern where each layer connects to all the preceding layers and the concatenation operation (rather than the addition operator in ResNet) to preserve and reuse features from earlier layers. 

That is, ResNet decomposes f into a simple linear term and a more complex nonlinear one.What if we wanted to capture (not necessarily add) information beyond two terms? One such solution is DenseNet 

![../_images/densenet-block.svg](https://d2l.ai/_images/densenet-block.svg)

<center>
    *Fig. 8.7.1* The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation.
</center>

As shown in [Fig. 8.7.1](https://d2l.ai/chapter_convolutional-modern/densenet.html#fig-densenet-block), the key difference between ResNet and DenseNet is that in the latter case outputs are *concatenated* (denoted by [,]) rather than added. 

In the end, all these functions are combined in MLP to reduce the number of features again. 

DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The final layer of such a chain is densely connected to all previous layers. The dense connections are shown in [Fig. 8.7.2](https://d2l.ai/chapter_convolutional-modern/densenet.html#fig-densenet).



![../_images/densenet.svg](https://d2l.ai/_images/densenet.svg)

The main components that comprise a DenseNet are *dense blocks* and *transition layers*. The former define how the inputs and outputs are concatenated, while the latter control the number of channels so that it is not too large, since the expansion x→[x,f1(x),f2([x,f1(x)]),…] can be quite high-dimensional.

```python
def conv_block(num_channels):
    return nn.Sequential(
        nn.LazyBatchNorm2d(), nn.ReLU(),
        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))
```

A *dense block* consists of multiple convolution blocks, each using the same number of output channels. In the forward propagation, however, we concatenate the input and output of each convolution block on the channel dimension. Lazy evaluation allows us to adjust the dimensionality automatically.

```python
class DenseBlock(nn.Module):
    def __init__(self, num_convs, num_channels):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(num_convs):
            layer.append(conv_block(num_channels))
        self.net = nn.Sequential(*layer)

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            # Concatenate input and output of each block along the channels
            X = torch.cat((X, Y), dim=1)
        return X
```

he number of convolution block channels controls the growth in the number of output channels relative to the number of input channels. This is also referred to as the *growth rate*.

---

Since each dense block will increase the number of channels, adding too many of them will lead to an excessively complex model. *transition layer* is used to control the complexity of the model. It reduces the number of channels by using a 1×1 convolution. Moreover, it halves the height and width via average pooling with a stride of 2.

```python
def transition_block(num_channels):
    return nn.Sequential(
        nn.LazyBatchNorm2d(), nn.ReLU(),
        nn.LazyConv2d(num_channels, kernel_size=1),
        nn.AvgPool2d(kernel_size=2, stride=2))
```

### DenseNet Model

DenseNet first uses the same single convolutional layer and max-pooling layer as in ResNet.

```python
@d2l.add_to_class(DenseNet)
def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),
             lr=0.1, num_classes=10):
    super(DenseNet, self).__init__()
    self.save_hyperparameters()
    self.net = nn.Sequential(self.b1())
    for i, num_convs in enumerate(arch):
        self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,
                                                          growth_rate))
        # The number of output channels in the previous dense block
        num_channels += num_convs * growth_rate
        # A transition layer that halves the number of channels is added
        # between the dense blocks
        if i != len(arch) - 1:
            num_channels //= 2
            self.net.add_module(f'tran_blk{i+1}', transition_block(
                num_channels))
    self.net.add_module('last', nn.Sequential(
        nn.LazyBatchNorm2d(), nn.ReLU(),
        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
        nn.LazyLinear(num_classes)))
    self.net.apply(d2l.init_cnn)
```

```python
model = DenseNet(lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
trainer.fit(model, data)
```

### Summary 

The main components that comprise DenseNet are dense blocks and transition layers. For the latter, we need to keep the dimensionality under control when composing the network by adding transition layers that shrink the number of channels again. 

Although these concatenation operations reuse features to achieve computational efficiency, unfortunately they lead to heavy GPU memory consumption.As a result, applying DenseNet may require more memory-efficient implementations that may increase training time 

### Designing Convolution Network Architectures

In the following we discuss an idea that is quite different to the quest for the *single best network*. It is computationally relatively inexpensive, it leads to scientific insights on the way, and it is quite effective in terms of the quality of outcomes.



Let’s review the strategy by Radosavovic *et al.* ([2020](https://d2l.ai/chapter_references/zreferences.html#id225)) to *design network design spaces*. The strategy combines the strength of manual design and NAS. It accomplishes this by operating on *distributions of networks* and optimizing the distributions in a way to obtain good performance for entire families of networks. he outcome of it are *RegNets*, specifically RegNetX and RegNetY, plus a range of guiding principles for the design of performant CNNs.



### The AnyNet Design Space

To begin, we need a template for the family of networks to explore. One of the commonalities of the designs in this chapter is that the networks consist of  ***a stem*, a *body* and a *head***. The stem performs **initial image processing**, often through convolutions with a larger window size. The body consists of multiple blocks, **carrying out the bulk of the transformations needed to go from raw images to object representations**. Lastly, the head **converts this into the desired outputs**, such as via a softmax regressor for multiclass classification. The body, in turn, consists of multiple stages, operating on the image at decreasing resolutions. In fact, both the stem and each subsequent stage quarter the spatial resolution. Lastly, each stage consists of one or more blocks. This pattern is common to all networks, from VGG to ResNeXt. Indeed, for the design of generic AnyNet networks, Radosavovic *et al.* ([2020](https://d2l.ai/chapter_references/zreferences.html#id225)) used the ResNeXt block of [Fig. 8.6.5](https://d2l.ai/chapter_convolutional-modern/resnet.html#fig-resnext-block).

![image-20240910203258819](../AppData/Roaming/Typora/typora-user-images/image-20240910203258819.png)

This seemingly generic design space provides us nonetheless with many parameters: we can set the block width (number of channels) c0,…c4, the depth (number of blocks) per stage d1,…d4, the bottleneck ratios k1,…k4, and the group widths (numbers of groups) g1,…g4. In total this adds up to 17 parameters, resulting in an unreasonably large number of configurations that would warrant exploring.



```python
class AnyNet(d2l.Classifier):
    def stem(self, num_channels):
        return nn.Sequential(
            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),
            nn.LazyBatchNorm2d(), nn.ReLU())
```

```python
@d2l.add_to_class(AnyNet)
def stage(self, depth, num_channels, groups, bot_mul):
    blk = []
    for i in range(depth):
        if i == 0:
            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,
                use_1x1conv=True, strides=2))
        else:
            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))
    return nn.Sequential(*blk)
```

```python
@d2l.add_to_class(AnyNet)
def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):
    super(AnyNet, self).__init__()
    self.save_hyperparameters()
    self.net = nn.Sequential(self.stem(stem_channels))
    for i, s in enumerate(arch):
        self.net.add_module(f'stage{i+1}', self.stage(*s))
    self.net.add_module('head', nn.Sequential(
        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
        nn.LazyLinear(num_classes)))
    self.net.apply(d2l.init_cnn)
```

 A better strategy would be to try to determine general guidelines of how the choices of parameters should be related. For instance, the bottleneck ratio, the number of channels, blocks, groups, or their change between layers should ideally be governed by a collection of simple rules. The approach in Radosavovic *et al.* ([2019](https://d2l.ai/chapter_references/zreferences.html#id363)) relies on the following four assumptions:

1. We assume that general design principles actually exist, so that many networks satisfying these requirements should offer good performance. Consequently, identifying a *distribution* over networks can be a sensible strategy. In other words, we assume that there are many good needles in the haystack.
2. We need not train networks to convergence before we can assess whether a network is good. Instead, it is sufficient to use the intermediate results as reliable guidance for final accuracy. Using (approximate) proxies to optimize an objective is referred to as multi-fidelity optimization ([Forrester *et al.*, 2007](https://d2l.ai/chapter_references/zreferences.html#id431)). Consequently, design optimization is carried out, based on the accuracy achieved after only a few passes through the dataset, reducing the cost significantly.
3. Results obtained at a smaller scale (for smaller networks) generalize to larger ones. Consequently, optimization is carried out for networks that are structurally similar, but with a smaller number of blocks, fewer channels, etc. Only in the end will we need to verify that the so-found networks also offer good performance at scale.
4. Aspects of the design can be approximately factorized so that it is possible to infer their effect on the quality of the outcome somewhat independently. In other words, the optimization problem is moderately easy.

![../_images/regnet-fig.png](https://d2l.ai/_images/regnet-fig.png)

Comparing error empirical distribution functions of design spaces.  $$\operatorname{AnyNet}_A$$is the original design space; $$\operatorname{AnyNet}_B$$ties the bottleneck ratios,  $$\operatorname{AnyNet}_C$$also ties group widths, 
 $$\operatorname{AnyNet}_D$$increases the network depth across stages. From left to right: (i) tying bottleneck ratios has no effect on performance; (ii) tying group widths has no effect on performance; (iii) increasing network widths (channels) across stages improves performance; (iv) increasing network depths across stages improves performance. Figure courtesy of Radosavovic et al. (2020).

Next we look for ways to reduce the multitude of potential choices for width and depth of the stages.
ltis a reasonable assumption that, as we go deeper, the number of channels should increase, i.e.,$c_i\geq c_{i-1}\left(w_{i+1}\geq w_i\text{ per their notation in Fig. 8.8.2), yielding AnyNetX}_D.\text{ Likewise, it is equally}\right)$reasonable to assume that as the stages progress, they should become deeper, i.e., $d_i\geq d_{i-1}$,
yielding $\mathrm{AnyNetX}_E.$ This can be experimentally verified in the third and fourth panel 

---



The resulting AnyNetXE design space consists of simple networks following easy-to-interpret design principles:

- Share the bottleneck ratio $ki=k$ for all stages $i$;
- Share the group width $gi=g$ for all stages $i$;
- Increase network width across stages: $ci≤ci+1$;
- Increase network depth across stages:$ di≤di+1$.

This leaves us with a fınal set of choices: how to pick the specifıc values for the above parameters of the eventual AnyNetX$_E$ design space. By studying the best-performing networks from the
distribution in AnyNetX $_E$ one can observe the following: the width of the network ideally increases linearly with the block index across the network, i.e., $c_j\approx c_0+c_aj$,where $j$ is the block index and slope $c_a>0$. Given that we get to choose a different block width only per stage, we arrive at a
piecewise constant function, engineered to match this dependence. Furthermore, experiments also show that a bottleneck ratio of $k=1$ performs best, i.e., we are advised not to use bottlenecks at all.

```python
class RegNetX32(AnyNet):
    def __init__(self, lr=0.1, num_classes=10):
        stem_channels, groups, bot_mul = 32, 16, 1
        depths, channels = (4, 6), (32, 80)
        super().__init__(
            ((depths[0], channels[0], groups, bot_mul),
             (depths[1], channels[1], groups, bot_mul)),
            stem_channels, lr, num_classes)
       #Best solution to the RegNetX32
```





## CV

### Anchor Boxes

In a training dataset, we consider each anchor box as a training example. In order to train an object detection model, we need *class* and *offset* labels for each anchor box, where the former is the class of the object relevant to the anchor box and the latter is the offset of the ground-truth bounding box relative to the anchor box. 

As we know, an object detection training set comes with labels for locations of *ground-truth bounding boxes* and classes of their surrounded objects. To label any generated *anchor box*, we refer to the labeled location and class of its *assigned* ground-truth bounding box that is closest to the anchor box. In the following, we describe an algorithm for assigning closest ground-truth bounding boxes to anchor boxes.

### 

> [!NOTE]
>
> ### Assigning Ground-Truth Bounding Boxes to Anchor Boxes
>
> Given an image, suppose that the anchor boxes are $A_1,A_2,\ldots,A_{n_a}$ and the ground-truth bounding boxes are $B_1,B_2,\ldots,B_{n_{b^{\prime}}}$ where $n_a\geq n_b$. Let's define a matrix $\mathbf{X}\in\mathbb{R}^{n_a\times n_b}$,whose element $x_ij$ in the $i^\mathrm{th}$ row and $j^\mathrm{th}$ column is the loU of the anchor box $A_i$ and the ground-truth bounding box $B_j.$ The algorithm consists of the following steps:
>
> 1.Find the largest element in matrix $\mathbf{X}$ and denote its row and column indices as $i_1$ and $j_1$,
> respectively. Then the ground-truth bounding box $B_{j_1}$ is assigned to the anchor box $A_{i_1}$. This is quite intuitive because $A_{i_1}$ and $B_{j_1}$ are the closest among all the pairs of anchor boxes and ground-truth bounding boxes. After the first assignment, discard all the elements in the $i_1^\mathrm{th}$ row and the $j_{1}^\mathrm{th}$ column in matrix $\mathbf{X}.$
> 2.Find the largest of the remaining elements in matrix $\mathbf{X}$ and denote its row and column indices
> as $i_2$ and $j_2$ respectively. We assign ground-truth bounding box $B_{j_2}$ to anchor box $A_{i_2}$ and
> discard all the elements in the $i_2^\mathrm{th}$ row and the $j_2^\mathrm{th}$ column in matrix $\mathbf{X}$
> 3. At this point, elements in two rows and two columns in matrix $\mathbf{X}$ have been discarded. We
>   proceed until all elements in $n_b$ columns in matrix $\mathbf{X}$ are discarded. At this time, we have
>   assigned a ground-truth bounding box to each of $n_b$ anchor boxes.
>
> 4. Only traverse through the remaining $n_a-n_b$ anchor boxes. For example, given any anchor
>   box $A_i$, fınd the ground-truth bounding box $B_j$ with the largest loU with $A_i$ throughout the $i^\mathrm{th}$ row of matrix $\mathbf{X}$, and assign $B_j$ to $A_i$ only if this IoU is greater than a predefined threshold.
>
> 5. ```python
>    #@save
>    def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
>        """Assign closest ground-truth bounding boxes to anchor boxes."""
>        num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
>        # Element x_ij in the i-th row and j-th column is the IoU of the anchor
>        # box i and the ground-truth bounding box j
>        jaccard = box_iou(anchors, ground_truth)
>        # Initialize the tensor to hold the assigned ground-truth bounding box for
>        # each anchor
>        anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,
>                                      device=device)
>        # Assign ground-truth bounding boxes according to the threshold
>        max_ious, indices = torch.max(jaccard, dim=1)
>        anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)
>        box_j = indices[max_ious >= iou_threshold]
>        anchors_bbox_map[anc_i] = box_j
>        col_discard = torch.full((num_anchors,), -1)
>        row_discard = torch.full((num_gt_boxes,), -1)
>        for _ in range(num_gt_boxes):
>            max_idx = torch.argmax(jaccard)  # Find the largest IoU
>            box_idx = (max_idx % num_gt_boxes).long()
>            anc_idx = (max_idx / num_gt_boxes).long()
>            anchors_bbox_map[anc_idx] = box_idx
>            jaccard[:, box_idx] = col_discard
>            jaccard[anc_idx, :] = row_discard
>        return anchors_bbox_map
>    ```



### Labeling Classes and Offsets

Given varying positions and sizes of different boxes in the dataset, we can apply transformations to those relative positions and sizes that may lead to more uniformly distributed offsets that are easier to fit. Here we describe a common transformation. Given the central coordinates of A and B as (xa,ya) and (xb,yb), their widths as wa and wb, and their heights as ha and hb, respectively. We may label the offset of A as
$$
\left(\frac{\frac{x_b-x_a}{w_a}-\mu_x}{\sigma_x},\frac{\frac{y_b-y_a}{h_a}-\mu_y}{\sigma_y},\frac{\log\frac{w_b}{w_a}-\mu_w}{\sigma_w},\frac{\log\frac{h_b}{h_a}-\mu_h}{\sigma_h}\right),
$$
where default values of the constants are μx=μy=μw=μh=0,σx=σy=0.1, and σw=σh=0.2. This transformation is implemented below in the `offset_boxes` function

```python
#@save
def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """Transform for anchor box offsets."""
    c_anc = d2l.box_corner_to_center(anchors)
    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
    offset = torch.cat([offset_xy, offset_wh], axis=1)
    return offset
```

If an anchor box is not assigned a ground-truth bounding box, we just label the class of the anchor box as “background”. Anchor boxes whose classes are background are often referred to as *negative* anchor boxes, and the rest are called *positive* anchor boxes. 

```python
#@save
def multibox_target(anchors, labels):
    """Label anchor boxes using ground-truth bounding boxes."""
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    batch_offset, batch_mask, batch_class_labels = [], [], []
    device, num_anchors = anchors.device, anchors.shape[0]
    for i in range(batch_size):
        label = labels[i, :, :]
        anchors_bbox_map = assign_anchor_to_bbox(
            label[:, 1:], anchors, device)
        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(
            1, 4)
        # Initialize class labels and assigned bounding box coordinates with
        # zeros
        class_labels = torch.zeros(num_anchors, dtype=torch.long,
                                   device=device)
        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,
                                  device=device)
        # Label classes of anchor boxes using their assigned ground-truth
        # bounding boxes. If an anchor box is not assigned any, we label its
        # class as background (the value remains zero)
        indices_true = torch.nonzero(anchors_bbox_map >= 0)
        bb_idx = anchors_bbox_map[indices_true]
        class_labels[indices_true] = label[bb_idx, 0].long() + 1
        assigned_bb[indices_true] = label[bb_idx, 1:]
        # Offset transformation
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask
        batch_offset.append(offset.reshape(-1))
        batch_mask.append(bbox_mask.reshape(-1))
        batch_class_labels.append(class_labels)
    bbox_offset = torch.stack(batch_offset)
    bbox_mask = torch.stack(batch_mask)
    class_labels = torch.stack(batch_class_labels)
    return (bbox_offset, bbox_mask, class_labels)
```
